{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TOPSIS Analysis for Text Generation Models\n",
        "==========================================\n",
        "This script ranks text generation models using TOPSIS\n",
        "(Technique for Order of Preference by Similarity to Ideal Solution).\n",
        "\n",
        "Models evaluated:\n",
        "GPT-2, GPT-Neo, T5, BART, LLaMA-2, Mistral\n",
        "\n",
        "Criteria:\n",
        "Quality → BLEU, ROUGE-L, BERTScore\n",
        "Efficiency → Latency, VRAM usage, Model size\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MODELS\n",
        "# --------------------------------------------------\n",
        "models = [\n",
        "    'GPT-2',\n",
        "    'GPT-Neo-1.3B',\n",
        "    'T5-base',\n",
        "    'BART-large',\n",
        "    'LLaMA-2-7B',\n",
        "    'Mistral-7B'\n",
        "]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DECISION MATRIX\n",
        "# (Example benchmark-based values)\n",
        "# --------------------------------------------------\n",
        "# Columns:\n",
        "# BLEU, ROUGE-L, BERTScore, Latency(ms), VRAM(GB), Model Size(MB)\n",
        "\n",
        "data = np.array([\n",
        "    [28.5, 31.2, 0.86, 40, 0.5, 500],    # GPT-2\n",
        "    [32.8, 35.6, 0.89, 85, 1.3, 2600],   # GPT-Neo\n",
        "    [36.5, 38.1, 0.91, 72, 0.9, 850],    # T5-base\n",
        "    [38.9, 40.3, 0.93, 95, 1.6, 1600],   # BART-large\n",
        "    [41.2, 42.5, 0.95, 120, 3.5, 4200],  # LLaMA-2\n",
        "    [42.8, 44.1, 0.96, 110, 3.0, 4100]   # Mistral\n",
        "])\n",
        "\n",
        "criteria = [\n",
        "    'BLEU Score',\n",
        "    'ROUGE-L',\n",
        "    'BERTScore',\n",
        "    'Latency (ms)',\n",
        "    'VRAM Usage (GB)',\n",
        "    'Model Size (MB)'\n",
        "]\n",
        "\n",
        "# True = higher is better\n",
        "# False = lower is better\n",
        "beneficial = [True, True, True, False, False, False]\n",
        "\n",
        "# Weights must sum to 1\n",
        "weights = np.array([0.25, 0.20, 0.20, 0.15, 0.10, 0.10])\n",
        "\n",
        "# --------------------------------------------------\n",
        "print(\"=\"*80)\n",
        "print(\"TOPSIS ANALYSIS FOR TEXT GENERATION MODELS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Display decision matrix\n",
        "df = pd.DataFrame(data, index=models, columns=criteria)\n",
        "print(\"Input Decision Matrix:\")\n",
        "print(tabulate(df, headers='keys', tablefmt='grid', floatfmt='.3f'))\n",
        "print()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 1: NORMALIZE\n",
        "# --------------------------------------------------\n",
        "print(\"Step 1: Normalizing matrix...\\n\")\n",
        "\n",
        "norm_divisors = np.sqrt(np.sum(data**2, axis=0))\n",
        "normalized_data = data / norm_divisors\n",
        "\n",
        "df_norm = pd.DataFrame(normalized_data, index=models, columns=criteria)\n",
        "print(tabulate(df_norm, headers='keys', tablefmt='grid', floatfmt='.4f'))\n",
        "print()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 2: WEIGHTED NORMALIZED MATRIX\n",
        "# --------------------------------------------------\n",
        "print(\"Step 2: Applying weights...\\n\")\n",
        "\n",
        "weighted_data = normalized_data * weights\n",
        "\n",
        "df_weighted = pd.DataFrame(weighted_data, index=models, columns=criteria)\n",
        "print(tabulate(df_weighted, headers='keys', tablefmt='grid', floatfmt='.4f'))\n",
        "print()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 3: IDEAL BEST & WORST\n",
        "# --------------------------------------------------\n",
        "print(\"Step 3: Finding ideal best and worst...\\n\")\n",
        "\n",
        "ideal_best = np.zeros(len(criteria))\n",
        "ideal_worst = np.zeros(len(criteria))\n",
        "\n",
        "for i in range(len(criteria)):\n",
        "    if beneficial[i]:\n",
        "        ideal_best[i] = np.max(weighted_data[:, i])\n",
        "        ideal_worst[i] = np.min(weighted_data[:, i])\n",
        "    else:\n",
        "        ideal_best[i] = np.min(weighted_data[:, i])\n",
        "        ideal_worst[i] = np.max(weighted_data[:, i])\n",
        "\n",
        "print(\"Ideal Best (A+):\", ideal_best)\n",
        "print(\"Ideal Worst (A-):\", ideal_worst)\n",
        "print()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 4: DISTANCE FROM IDEAL\n",
        "# --------------------------------------------------\n",
        "print(\"Step 4: Calculating separation measures...\\n\")\n",
        "\n",
        "dist_best = np.sqrt(np.sum((weighted_data - ideal_best)**2, axis=1))\n",
        "dist_worst = np.sqrt(np.sum((weighted_data - ideal_worst)**2, axis=1))\n",
        "\n",
        "sep_df = pd.DataFrame({\n",
        "    \"Model\": models,\n",
        "    \"Distance from Best\": dist_best,\n",
        "    \"Distance from Worst\": dist_worst\n",
        "})\n",
        "print(tabulate(sep_df, headers='keys', tablefmt='grid', floatfmt='.6f', showindex=False))\n",
        "print()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 5: TOPSIS SCORE\n",
        "# --------------------------------------------------\n",
        "print(\"Step 5: Computing TOPSIS scores...\\n\")\n",
        "\n",
        "scores = dist_worst / (dist_best + dist_worst)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": models,\n",
        "    \"TOPSIS Score\": scores,\n",
        "    \"Rank\": np.argsort(-scores) + 1\n",
        "}).sort_values(\"Rank\")\n",
        "\n",
        "print(tabulate(results, headers='keys', tablefmt='grid', floatfmt='.6f', showindex=False))\n",
        "print()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# FINAL RANKING\n",
        "# --------------------------------------------------\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL RANKING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for _, row in results.iterrows():\n",
        "    print(f\"{int(row['Rank'])}. {row['Model']:<15} Score: {row['TOPSIS Score']:.6f}\")\n",
        "\n",
        "best_model = results.iloc[0]['Model']\n",
        "print(\"\\nRECOMMENDED MODEL:\", best_model)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# EXPORT RESULTS\n",
        "# --------------------------------------------------\n",
        "results.to_csv(\"results/topsis_ranking_generation.csv\", index=False)\n",
        "\n",
        "breakdown = pd.DataFrame({\n",
        "    \"Model\": models,\n",
        "    \"Distance_from_Best\": dist_best,\n",
        "    \"Distance_from_Worst\": dist_worst,\n",
        "    \"TOPSIS_Score\": scores,\n",
        "    \"Rank\": np.argsort(-scores) + 1\n",
        "})\n",
        "breakdown.to_csv(\"results/topsis_breakdown_generation.csv\", index=False)\n",
        "\n",
        "print(\"\\nResults saved in 'results/' folder.\")\n",
        "print(\"Script completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH2PjPr4wsOf",
        "outputId": "7a0acd29-336d-4750-97e3-8b43e0fb38f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TOPSIS ANALYSIS FOR TEXT GENERATION MODELS\n",
            "================================================================================\n",
            "\n",
            "Input Decision Matrix:\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "|              |   BLEU Score |   ROUGE-L |   BERTScore |   Latency (ms) |   VRAM Usage (GB) |   Model Size (MB) |\n",
            "+==============+==============+===========+=============+================+===================+===================+\n",
            "| GPT-2        |       28.500 |    31.200 |       0.860 |         40.000 |             0.500 |           500.000 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| GPT-Neo-1.3B |       32.800 |    35.600 |       0.890 |         85.000 |             1.300 |          2600.000 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| T5-base      |       36.500 |    38.100 |       0.910 |         72.000 |             0.900 |           850.000 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| BART-large   |       38.900 |    40.300 |       0.930 |         95.000 |             1.600 |          1600.000 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| LLaMA-2-7B   |       41.200 |    42.500 |       0.950 |        120.000 |             3.500 |          4200.000 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| Mistral-7B   |       42.800 |    44.100 |       0.960 |        110.000 |             3.000 |          4100.000 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "\n",
            "Step 1: Normalizing matrix...\n",
            "\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "|              |   BLEU Score |   ROUGE-L |   BERTScore |   Latency (ms) |   VRAM Usage (GB) |   Model Size (MB) |\n",
            "+==============+==============+===========+=============+================+===================+===================+\n",
            "| GPT-2        |       0.3135 |    0.3276 |      0.3827 |         0.1797 |            0.0970 |            0.0747 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| GPT-Neo-1.3B |       0.3608 |    0.3739 |      0.3961 |         0.3819 |            0.2522 |            0.3887 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| T5-base      |       0.4015 |    0.4001 |      0.4050 |         0.3235 |            0.1746 |            0.1271 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| BART-large   |       0.4279 |    0.4232 |      0.4139 |         0.4268 |            0.3105 |            0.2392 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| LLaMA-2-7B   |       0.4532 |    0.4463 |      0.4228 |         0.5392 |            0.6791 |            0.6279 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| Mistral-7B   |       0.4708 |    0.4631 |      0.4272 |         0.4942 |            0.5821 |            0.6129 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "\n",
            "Step 2: Applying weights...\n",
            "\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "|              |   BLEU Score |   ROUGE-L |   BERTScore |   Latency (ms) |   VRAM Usage (GB) |   Model Size (MB) |\n",
            "+==============+==============+===========+=============+================+===================+===================+\n",
            "| GPT-2        |       0.0784 |    0.0655 |      0.0765 |         0.0270 |            0.0097 |            0.0075 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| GPT-Neo-1.3B |       0.0902 |    0.0748 |      0.0792 |         0.0573 |            0.0252 |            0.0389 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| T5-base      |       0.1004 |    0.0800 |      0.0810 |         0.0485 |            0.0175 |            0.0127 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| BART-large   |       0.1070 |    0.0846 |      0.0828 |         0.0640 |            0.0310 |            0.0239 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| LLaMA-2-7B   |       0.1133 |    0.0893 |      0.0846 |         0.0809 |            0.0679 |            0.0628 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "| Mistral-7B   |       0.1177 |    0.0926 |      0.0854 |         0.0741 |            0.0582 |            0.0613 |\n",
            "+--------------+--------------+-----------+-------------+----------------+-------------------+-------------------+\n",
            "\n",
            "Step 3: Finding ideal best and worst...\n",
            "\n",
            "Ideal Best (A+): [0.11771163 0.09262396 0.08544903 0.02695874 0.00970188 0.00747498]\n",
            "Ideal Worst (A-): [0.07838274 0.06552988 0.07654809 0.08087621 0.06791317 0.06278981]\n",
            "\n",
            "Step 4: Calculating separation measures...\n",
            "\n",
            "+--------------+----------------------+-----------------------+\n",
            "| Model        |   Distance from Best |   Distance from Worst |\n",
            "+==============+======================+=======================+\n",
            "| GPT-2        |             0.048581 |              0.096723 |\n",
            "+--------------+----------------------+-----------------------+\n",
            "| GPT-Neo-1.3B |             0.057100 |              0.056421 |\n",
            "+--------------+----------------------+-----------------------+\n",
            "| T5-base      |             0.032118 |              0.082546 |\n",
            "+--------------+----------------------+-----------------------+\n",
            "| BART-large   |             0.047812 |              0.066153 |\n",
            "+--------------+----------------------+-----------------------+\n",
            "| LLaMA-2-7B   |             0.096886 |              0.042982 |\n",
            "+--------------+----------------------+-----------------------+\n",
            "| Mistral-7B   |             0.086461 |              0.050019 |\n",
            "+--------------+----------------------+-----------------------+\n",
            "\n",
            "Step 5: Computing TOPSIS scores...\n",
            "\n",
            "+--------------+----------------+--------+\n",
            "| Model        |   TOPSIS Score |   Rank |\n",
            "+==============+================+========+\n",
            "| GPT-Neo-1.3B |       0.497009 |      1 |\n",
            "+--------------+----------------+--------+\n",
            "| BART-large   |       0.580469 |      2 |\n",
            "+--------------+----------------+--------+\n",
            "| GPT-2        |       0.665662 |      3 |\n",
            "+--------------+----------------+--------+\n",
            "| T5-base      |       0.719894 |      4 |\n",
            "+--------------+----------------+--------+\n",
            "| Mistral-7B   |       0.366492 |      5 |\n",
            "+--------------+----------------+--------+\n",
            "| LLaMA-2-7B   |       0.307305 |      6 |\n",
            "+--------------+----------------+--------+\n",
            "\n",
            "================================================================================\n",
            "FINAL RANKING\n",
            "================================================================================\n",
            "1. GPT-Neo-1.3B    Score: 0.497009\n",
            "2. BART-large      Score: 0.580469\n",
            "3. GPT-2           Score: 0.665662\n",
            "4. T5-base         Score: 0.719894\n",
            "5. Mistral-7B      Score: 0.366492\n",
            "6. LLaMA-2-7B      Score: 0.307305\n",
            "\n",
            "RECOMMENDED MODEL: GPT-Neo-1.3B\n",
            "\n",
            "Results saved in 'results/' folder.\n",
            "Script completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Visualization Script for TOPSIS Text Generation Analysis\n",
        "========================================================\n",
        "Generates bar charts, radar charts, heatmaps, and ranking visuals.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import pi\n",
        "import os\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MODELS & METRICS\n",
        "# --------------------------------------------------\n",
        "models = [\n",
        "    'GPT-2',\n",
        "    'GPT-Neo-1.3B',\n",
        "    'T5-base',\n",
        "    'BART-large',\n",
        "    'LLaMA-2-7B',\n",
        "    'Mistral-7B'\n",
        "]\n",
        "\n",
        "criteria = [\n",
        "    'BLEU Score',\n",
        "    'ROUGE-L',\n",
        "    'BERTScore',\n",
        "    'Latency (ms)',\n",
        "    'VRAM Usage (GB)',\n",
        "    'Model Size (MB)'\n",
        "]\n",
        "\n",
        "data = np.array([\n",
        "    [28.5, 31.2, 0.86, 40, 0.5, 500],\n",
        "    [32.8, 35.6, 0.89, 85, 1.3, 2600],\n",
        "    [36.5, 38.1, 0.91, 72, 0.9, 850],\n",
        "    [38.9, 40.3, 0.93, 95, 1.6, 1600],\n",
        "    [41.2, 42.5, 0.95, 120, 3.5, 4200],\n",
        "    [42.8, 44.1, 0.96, 110, 3.0, 4100]\n",
        "])\n",
        "\n",
        "# Load TOPSIS results from previous script\n",
        "results_df = pd.read_csv('results/topsis_ranking_generation.csv')\n",
        "topsis_scores = results_df['TOPSIS Score'].values\n",
        "ranks = results_df['Rank'].values\n",
        "\n",
        "print(\"Generating visualizations...\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. TOPSIS SCORE BAR CHART\n",
        "# --------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "bars = ax.barh(models, topsis_scores)\n",
        "\n",
        "for bar, score in zip(bars, topsis_scores):\n",
        "    ax.text(bar.get_width()+0.005,\n",
        "            bar.get_y()+bar.get_height()/2,\n",
        "            f'{score:.4f}', va='center')\n",
        "\n",
        "ax.set_title('TOPSIS Score Comparison (Text Generation Models)', fontweight='bold')\n",
        "ax.set_xlabel('TOPSIS Score')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/topsis_scores_generation.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. METRIC COMPARISON\n",
        "# --------------------------------------------------\n",
        "fig, axes = plt.subplots(2,3, figsize=(16,10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, criterion in enumerate(criteria):\n",
        "    ax = axes[i]\n",
        "    bars = ax.bar(models, data[:, i])\n",
        "    ax.set_title(criterion, fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.suptitle('Metrics Comparison Across Models', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/metrics_comparison_generation.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. RADAR CHART\n",
        "# --------------------------------------------------\n",
        "radar_data = np.zeros_like(data, dtype=float)\n",
        "\n",
        "# Normalize (invert cost metrics)\n",
        "cost_indices = [3,4,5]\n",
        "\n",
        "for i in range(data.shape[1]):\n",
        "    if i in cost_indices:\n",
        "        radar_data[:, i] = 1 - (data[:, i] - data[:, i].min()) / (data[:, i].max() - data[:, i].min())\n",
        "    else:\n",
        "        radar_data[:, i] = (data[:, i] - data[:, i].min()) / (data[:, i].max() - data[:, i].min())\n",
        "\n",
        "angles = [n/float(len(criteria))*2*pi for n in range(len(criteria))]\n",
        "angles += angles[:1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10), subplot_kw=dict(polar=True))\n",
        "\n",
        "for idx, model in enumerate(models):\n",
        "    values = radar_data[idx].tolist()\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, linewidth=2, label=model)\n",
        "    ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(criteria)\n",
        "ax.set_title('Normalized Performance Radar Chart', fontweight='bold')\n",
        "ax.legend(bbox_to_anchor=(1.3,1.1))\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/radar_generation.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. HEATMAP\n",
        "# --------------------------------------------------\n",
        "norm_divisors = np.sqrt(np.sum(data**2, axis=0))\n",
        "normalized_data = data / norm_divisors\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(normalized_data, annot=True, cmap='YlOrRd',\n",
        "            xticklabels=criteria, yticklabels=models)\n",
        "\n",
        "plt.title('Normalized Decision Matrix Heatmap', fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/heatmap_generation.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5. FINAL RANKING CHART\n",
        "# --------------------------------------------------\n",
        "sorted_idx = np.argsort(ranks)\n",
        "sorted_models = [models[i] for i in sorted_idx]\n",
        "sorted_scores = topsis_scores[sorted_idx]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "bars = ax.barh(sorted_models, sorted_scores)\n",
        "\n",
        "for bar, score in zip(bars, sorted_scores):\n",
        "    ax.text(bar.get_width()+0.005,\n",
        "            bar.get_y()+bar.get_height()/2,\n",
        "            f'{score:.4f}', va='center')\n",
        "\n",
        "ax.set_title('Final Ranking (TOPSIS)', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/final_ranking_generation.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nAll visualizations created successfully!\")\n",
        "print(\"Check the results/ folder.\")"
      ],
      "metadata": {
        "id": "D-N9gdX5yF_4",
        "outputId": "accd184c-eff8-4b4c-d41f-2134b44642b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating visualizations...\n",
            "\n",
            "All visualizations created successfully!\n",
            "Check the results/ folder.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}